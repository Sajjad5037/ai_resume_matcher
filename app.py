import os 
import streamlit as st
from pypdf import PdfReader
from docx import Document 
import io
import pandas as pd
import json 
#from openai import OpenAI
import re
import google.generativeai as genai
import mimetypes

def to_gemini_part(uploaded_file):
    uploaded_file.seek(0)

    # Guess MIME type from filename (more reliable than browser)
    mime_type, _ = mimetypes.guess_type(uploaded_file.name)

    # Hard fallback for PDFs
    if uploaded_file.name.lower().endswith(".pdf"):
        mime_type = "application/pdf"

    # Final fallback (should almost never be used)
    if not mime_type:
        mime_type = uploaded_file.type

    return {
        "mime_type": mime_type,
        "data": uploaded_file.read(),
    }



 
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    st.error("GOOGLE_API_KEY is NOT loaded. Check Streamlit Secrets.")
    st.stop()



# ----------------------------
# OpenAI setup
# ----------------------------
#if not os.getenv("OPENAI_API_KEY"):
#    st.error("OPENAI_API_KEY is NOT loaded. Check Streamlit Secrets.")
#    st.stop()

#client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ----------------------------
# App Config
# ----------------------------
st.set_page_config(
    page_title="AI Resume Matcher (new)",
    layout="centered"
)
if "results" not in st.session_state:
    st.session_state.results = []

if "explanations" not in st.session_state:
    st.session_state.explanations = {}
if "explain_open" not in st.session_state:
    st.session_state.explain_open = {}

if "cvs" not in st.session_state:
    st.session_state.cvs = None
if "active_candidate" not in st.session_state:
    st.session_state.active_candidate = None

    
st.success("Gemini API key loaded successfully.")


st.title("AI Resume Matcher (old)")



# ----------------------------
# Model Selection & Diagnostics
# ----------------------------
try:
    # This fetches the actual list from Google
    available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
except Exception as e:
    st.error(f"Could not fetch models: {e}")
    available_models = []

# ----------------------------
# Model Selection
# ----------------------------
MODEL_OPTIONS = {
    "Gemini 3 Flash (Preview)": "models/gemini-3-flash-preview"
}
selected_model_label = st.selectbox(
    "Select AI model",
    options=list(MODEL_OPTIONS.keys()),
    index=0
)

SELECTED_MODEL = MODEL_OPTIONS[selected_model_label]

st.caption(f"Using model: `{SELECTED_MODEL}`")

st.write("Upload a candidate CV to see which jobs are most likely to result in an offer.")
st.write(
    "ğŸ“Œ For best accuracy, upload CVs in DOCX or text-based PDF format. "
    "Scanned PDFs may reduce matching quality."
)


def extract_cv_text_from_uploaded_file(uploaded_file) -> str:
    uploaded_file.seek(0)
    file_type = uploaded_file.type

    # ----------------
    # PDF (SAFE)
    # ----------------
    if file_type == "application/pdf":
        reader = PdfReader(uploaded_file)
        text = []
        failed_pages = 0

        for page in reader.pages:
            try:
                page_text = page.extract_text()
                if page_text and page_text.strip():
                    text.append(page_text)
            except Exception:
                failed_pages += 1
                continue

        if failed_pages > 0:
            st.warning(
                f"âš ï¸ {uploaded_file.name}: {failed_pages} page(s) could not be read and were skipped."
            )

        return "\n".join(text)

    # ----------------
    # DOCX
    # ----------------
    if file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = Document(io.BytesIO(uploaded_file.read()))
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())

    # ----------------
    # XLSX
    # ----------------
    if file_type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
        df_dict = pd.read_excel(uploaded_file, sheet_name=None)
        blocks = []
        for sheet, df in df_dict.items():
            blocks.append(f"ã€Sheet: {sheet}ã€‘")
            for _, row in df.iterrows():
                row_text = " ".join(str(v) for v in row.values if not pd.isna(v))
                if row_text.strip():
                    blocks.append(row_text)
        return "\n".join(blocks)

    return ""


def safe_parse_json(text):
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("No valid JSON found")
    return json.loads(text[start:end + 1])



def aggregate_candidate_cv_text(uploaded_files):
    combined_blocks = []
    filenames = []

    for file in uploaded_files:
        try:
            text = extract_cv_text_from_uploaded_file(file)
        except Exception:
            st.error(f"âŒ Failed to process {file.name}")
            continue

        if text.strip():
            combined_blocks.append(f"\n\n--- {file.name} ---\n{text}")
            filenames.append(file.name)
        else:
            st.info(
                f"â„¹ï¸ {file.name} contains little or no readable text."
            )

    return {
        "cv_text": "\n".join(combined_blocks),
        "filenames": filenames
    }

def generate_explanation(job, evaluation):


    score = evaluation["score"]

    # Create a prompt based on client requirements
    prompt = f"""
Return ONLY valid JSON.
Do not include markdown.
Do not include any text outside JSON.
Do not add extra keys.

ã‚ãªãŸã¯ã€æ¡ç”¨ãƒ»æ›¸é¡é¸è€ƒã®å®Ÿå‹™çµŒé¨“ãŒè±Šå¯Œãªäººæã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œè©•ä¾¡çµæœã€ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€
ãã®åˆ¤æ–­ç†ç”±ã‚’æ¡ç”¨æ‹…å½“è€…å‘ã‘ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

â€» ã‚ãªãŸã¯å±¥æ­´æ›¸ã‚’å†è©•ä¾¡ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
â€» æ–°ã—ã„åˆ¤æ–­ã‚„æ¨æ¸¬ã‚’è¡Œã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
â€» ä¸‹è¨˜ã®è©•ä¾¡çµæœã‚’èª¬æ˜ãƒ»è¨€èªåŒ–ã™ã‚‹ã“ã¨ã ã‘ãŒç›®çš„ã§ã™ã€‚

å½“è©²è·ç¨®ã«ãŠã‘ã‚‹å€™è£œè€…ã®å†…å®šå¯èƒ½æ€§ã«ã¤ã„ã¦ã€æ¡ç”¨æ‹…å½“è€…å‘ã‘ã«
å®¢è¦³çš„ã‹ã¤ä¸å¯§ãªè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å¿…é ˆãƒ«ãƒ¼ãƒ«ã€‘
- å‡ºåŠ›ã¯ã™ã¹ã¦æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- è‡ªç„¶ã§å®¢è¦³çš„ãªãƒ“ã‚¸ãƒã‚¹æ—¥æœ¬èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- ç®‡æ¡æ›¸ãã¯ä½¿ç”¨ã›ãšã€æ–‡ç« å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- AIã€ãƒ¢ãƒ‡ãƒ«ã€ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹è¨€åŠã¯ç¦æ­¢ã§ã™ã€‚
- å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¿…ãš1æ–‡ä»¥ä¸Šã®å®Œå…¨ãªæ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- å†…å®¹ãŒä¸æ˜ãªå ´åˆã§ã‚‚ã€ç©ºæ¬„ã«ã¯ã›ãšã€è©•ä¾¡æ–‡ã¨ã—ã¦æˆç«‹ã•ã›ã¦ãã ã•ã„ã€‚

ã€è©•ä¾¡ã®å‰æã€‘
- è©•ä¾¡ã¯ã€æä¾›ã•ã‚ŒãŸCVã«æ˜ç¤ºçš„ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ãã ã•ã„ã€‚
- æ¨æ¸¬ã‚„è£œå®Œã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚
- å„è©•ä¾¡ã¯ã€æ¡ç”¨æ‹…å½“è€…ãŒç¤¾å†…å…±æœ‰ã§ãã‚‹èª¬æ˜ã¨ã—ã¦æˆç«‹ã™ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚

ã€è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
- å¿…é ˆè¦ä»¶ã®è©•ä¾¡ï¼š{evaluation["criteria"]["must_have_requirements"]}
- æ­“è¿è¦ä»¶ã®è©•ä¾¡ï¼š{evaluation["criteria"]["preferred_requirements"]}
- è·å‹™å†…å®¹ã¨ã®é©åˆæ€§ï¼š{evaluation["criteria"]["role_alignment"]}
- æƒ³å®šå†…å®šç¢ºç‡ï¼š{score}ï¼…

ã€å‡ºåŠ›JSONå½¢å¼ï¼ˆå³å®ˆï¼‰ã€‘
{{
  "SUMMARY": "",
  "MUST_HAVE": "",
  "PREFERRED": "",
  "ALIGNMENT": ""
}}


ã€è·å‹™å†…å®¹ã€‘
{job["job_context"][:1200]}
"""

    model = genai.GenerativeModel(SELECTED_MODEL)

    response = model.generate_content(
        prompt,
        generation_config={
            "temperature": 0.3,
            "max_output_tokens": 900,
        }
    )


    try:
        return safe_parse_json(response.text)

    except Exception:
        # Hard fallback in case of error
        if score == 0:
            return {
                "SUMMARY": "æä¾›ã•ã‚ŒãŸå±¥æ­´æ›¸ã®å†…å®¹ã‹ã‚‰ã¯ã€å½“è©²è·ç¨®ã«ãŠã„ã¦å†…å®šã«è‡³ã‚‹å¯èƒ½æ€§ã¯ç¾æ™‚ç‚¹ã§ã¯ä½ã„ã¨åˆ¤æ–­ã•ã‚Œã¾ã™ã€‚",
                "MUST_HAVE": "å¿…é ˆè¦ä»¶ã«è©²å½“ã™ã‚‹æ˜ç¢ºãªçµŒé¨“ã‚„æ ¹æ‹ ãŒå±¥æ­´æ›¸ä¸Šã§ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "PREFERRED": "æ­“è¿è¦ä»¶ã«ã¤ã„ã¦ã‚‚ã€ç›´æ¥çš„ãªé©åˆæ€§ã¯é™å®šçš„ã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚",
                "ALIGNMENT": "è·å‹™å†…å®¹ã¨ã®ç›´æ¥çš„ãªä¸€è‡´ã¯ç¢ºèªã§ããšã€æ¥­å‹™é©åˆæ€§ã¯ä½ã„ã¨åˆ¤æ–­ã•ã‚Œã¾ã™ã€‚"
            }

        return {
            "SUMMARY": "å±¥æ­´æ›¸ã®å†…å®¹ã‚’ç·åˆçš„ã«åˆ¤æ–­ã™ã‚‹ã¨ã€ä¸€éƒ¨ã«è©•ä¾¡å¯èƒ½ãªè¦ç´ ã¯ã‚ã‚‹ã‚‚ã®ã®ã€å†…å®šå¯èƒ½æ€§ã¯é™å®šçš„ã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚",
            "MUST_HAVE": "å¿…é ˆè¦ä»¶ã«ã¤ã„ã¦ã¯ä¸€éƒ¨æº€ãŸã—ã¦ã„ã‚‹å¯èƒ½æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€ååˆ†ãªæ ¹æ‹ ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
            "PREFERRED": "æ­“è¿è¦ä»¶ã«ã¤ã„ã¦ã¯é™å®šçš„ãªé©åˆæ€§ãŒç¢ºèªã§ãã¾ã™ã€‚",
            "ALIGNMENT": "æ¥­å‹™å†…å®¹ã¨ã®è¦ªå’Œæ€§ã¯ä¸€å®šç¨‹åº¦ç¢ºèªã§ãã¾ã™ãŒã€æ±ºå®šçš„ã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚"
        }


def generate_with_retry(model, prompt, candidate_files, retries=1):
    last_error = None

    for attempt in range(1, retries + 1):
        response = model.generate_content(
            prompt,  # âœ… now defined via parameter
            generation_config={
                "temperature": 0.3,
                "max_output_tokens": 900,
            }
        )

        candidate = response.candidates[0]
        raw = candidate.content.parts[0].text

        try:
            parsed = extract_json(raw)
            return parsed, raw
        except Exception as e:
            last_error = e

    raise ValueError(f"Failed after retries: {last_error}")

def extract_json(text):
    # Find the first opening brace
    start = text.find("{")
    if start == -1:
        raise ValueError("No JSON object found in model output")

    # Find the last closing brace
    end = text.rfind("}")
    if end == -1 or end <= start:
        raise ValueError("Incomplete JSON object in model output")

    json_str = text[start:end + 1]

    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON returned by model: {e}")
# ----------------------------
# Helpers
# ----------------------------
def extract_text(uploaded_file):
    if uploaded_file.type == "application/pdf":
        reader = PdfReader(uploaded_file)
        text = []
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text.append(page_text)
        return "\n".join(text)

    if uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = Document(io.BytesIO(uploaded_file.read()))
        return "\n".join(p.text for p in doc.paragraphs)

    return ""


def get_title(row):
    for key in ["title", "position"]:
        if key in row.index:
            value = row[key]
            if not pd.isna(value) and str(value).strip():
                return str(value).strip()
    return "Unknown Role"


def get_available_jobs(df: pd.DataFrame):
    """
    Builds structured job contexts from uploaded Excel file
    """
    df.columns = df.columns.astype(str).str.strip()

    jobs = []

    def safe(value):
        if pd.isna(value):
            return ""
        return str(value).strip()

    for _, row in df.iterrows():
        title = get_title(row)

        job_context_parts = [
            f"Job Title: {safe(row.get('title'))}",
            f"Position: {safe(row.get('position'))}",
            f"Industry: {safe(row.get('job_industry'))}",
            f"Job Type: {safe(row.get('job_type'))}",
            f"Location: {safe(row.get('location'))}",
            f"Job Description: {safe(row.get('job_content'))}",
            f"Required Experience: {safe(row.get('required_experience'))}",
            f"Desired Experience: {safe(row.get('desired_experience'))}",
            f"Target Candidate: {safe(row.get('target_candidate'))}",
            f"Education: {safe(row.get('education'))}",
            f"Eligibility Details: {safe(row.get('eligibility_details'))}",
        ]

        job_context = "\n".join(
            part for part in job_context_parts if part.split(": ", 1)[1]
        )

        jobs.append({
            "job_id": safe(row.get("job_url")),
            "title": title,
            "job_context": job_context,
            "company_name": safe(row.get("company_name")),
            "passrate_for_doc_screening": safe(row.get("passrate_for_doc_screening")),
            "documents_to_job_offer_ratio": safe(row.get("documents_to_job_offer_ratio")),
            "fee": safe(row.get("fee")),
        })

    return jobs


def ai_match_job(candidate_files, job, model_name):

    
    prompt = f"""
Return ONLY valid JSON.
No markdown.
No explanations.
No text outside JSON.

All string values MUST be single-line.
Do NOT include newline characters inside strings.

ã‚ãªãŸã¯ã€æ›¸é¡é¸è€ƒã‚’æ‹…å½“ã™ã‚‹æ¡ç”¨å®Ÿå‹™è€…ã§ã™ã€‚
ä»¥ä¸‹ã®å±¥æ­´æ›¸ï¼ˆCVï¼‰ã¨è·å‹™å†…å®¹ã‚’æ¯”è¼ƒã—ã€
CVã«æ˜ç¤ºçš„ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
- æ¨æ¸¬ã‚„è£œå®Œã¯ç¦æ­¢ã§ã™ã€‚
- é–“æ¥çš„ãƒ»æ±ç”¨çš„ãªé–¢é€£çµŒé¨“ãŒç¢ºèªã§ãã‚‹å ´åˆã¯ã€Œâ–³ã€ã¨ã—ã¦ãã ã•ã„ã€‚
- CV ã«é–¢é€£ã™ã‚‹æ ¹æ‹ ãŒä¸€åˆ‡ç¢ºèªã§ããªã„å ´åˆã®ã¿ã€ŒÃ—ã€ã¨ã—ã¦ãã ã•ã„ã€‚
- ä¸è¶³ã‚„æœªçµŒé¨“ã‚’æ–­å®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

ã€è©•ä¾¡è¨˜å·ã€‘
â—‹ï¼šç›´æ¥çš„ã‹ã¤è·å‹™é–¢é€£æ€§ã®é«˜ã„çµŒé¨“ãŒç¢ºèªã§ãã‚‹  
â–³ï¼šé–“æ¥çš„ãƒ»æ±ç”¨çš„ãƒ»é™å®šçš„ãªé–¢é€£çµŒé¨“ãŒç¢ºèªã§ãã‚‹  
Ã—ï¼šé–¢é€£ã™ã‚‹çµŒé¨“ã‚„æ ¹æ‹ ãŒä¸€åˆ‡ç¢ºèªã§ããªã„  

ã€å‡ºåŠ›JSONå½¢å¼ï¼ˆå³å®ˆï¼‰ã€‘
{
  "score": 0,
  "criteria": {
    "must_have_requirements": "â—‹|â–³|Ã—",
    "preferred_requirements": "â—‹|â–³|Ã—",
    "role_alignment": "â—‹|â–³|Ã—"
  }
}

ã€ã‚¹ã‚³ã‚¢ç®—å‡ºãƒ«ãƒ¼ãƒ«ã€‘
- score ã¯ 0 ã‹ã‚‰ 100 ã®æ•´æ•°ã§è¿”ã—ã¦ãã ã•ã„ã€‚
- ä¸Šè¨˜ criteria ã®è©•ä¾¡çµæœã‚’ç·åˆã—ã¦ score ã‚’ç®—å‡ºã—ã¦ãã ã•ã„ã€‚
- CV ã«æ˜ç¤ºçš„ãªæ ¹æ‹ ãŒã»ã¨ã‚“ã©ç¢ºèªã§ããªã„å ´åˆã¯ã€ä½ã„ score ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

ã€è·å‹™å†…å®¹ã€‘
{job["job_context"][:1500]}
"""



    try:
        model = genai.GenerativeModel(model_name)

        response = model.generate_content(
           [prompt, *candidate_files],
           generation_config={
               "temperature": 0.3,
               "max_output_tokens": 900,
           }
       )

        
        # Log candidate count and content parts
        if not response.candidates:
            raise ValueError("Gemini returned no candidates")
        
        content_parts = response.candidates[0].content.parts
        
        st.caption(
            f"ğŸ§  Gemini response parts: {len(content_parts)} "
            f"(includes prompt + {len(candidate_files)} document(s))"
        )
        
        raw = response.text
        parsed = extract_json(raw)
        # Defensive normalization
        if not isinstance(parsed.get("score"), int):
            parsed["score"] = 0
        # ğŸ” Heuristic warning: likely ingestion / readability issue
        if (
            parsed.get("score", 0) == 0 and
            all(v == "Ã—" for v in parsed.get("criteria", {}).values())
        ):
            st.warning(
                "âš ï¸ The evaluation returned no matching signals. "
                "This may indicate the CV content was not fully readable by the model."
            )



        return {
            "ok": True,
            "data": parsed,
            "raw": raw
        }

    except Exception as e:
        return {
            "ok": False,
            "error": str(e),
            "raw": raw if 'raw' in locals() else None,
            "data": {
                "score": 0,
                "criteria": {
                    "must_have_requirements": "Ã—",
                    "preferred_requirements": "Ã—",
                    "role_alignment": "Ã—"
                }
            }
        }



    

# ----------------------------
# UI
# ----------------------------
uploaded_cvs = st.file_uploader(
    "Upload CV files (PDF / DOCX / XLSX)",
    type=["pdf", "docx", "xlsx"],
    accept_multiple_files=True,
    key="cv_files"
)

jobs_file = st.file_uploader(
    "Upload jobs Excel file",
    type=["xlsx"],
    key="jobs_excel"
)


# ğŸ”¹ ADD THIS BLOCK HERE (exactly here)
if uploaded_cvs:
    st.success("CV files uploaded")

    st.session_state.cvs = uploaded_cvs

    st.info(f"{len(uploaded_cvs)} CVs uploaded")
    
if uploaded_cvs and jobs_file and st.button("Evaluate CVs"):

    jobs_df = pd.read_excel(jobs_file)
    jobs = get_available_jobs(jobs_df)

    folder_results = []
    
    status = st.empty()
    progress = st.progress(0)

    # ğŸ”¹ Aggregate ALL CVs into ONE candidate
    candidate_files = []

    st.subheader("ğŸ“ Document ingestion log")
    
    for f in st.session_state.cvs:
        part = to_gemini_part(f)
    
        candidate_files.append(part)
    
        # UI log (very important for debugging)
        st.code(
            {
                "filename": f.name,
                "detected_mime_type": part["mime_type"],
                "file_size_kb": round(len(part["data"]) / 1024, 2),
            },
            language="json"
        )


    cv_files = [f.name for f in st.session_state.cvs]
    st.session_state.candidate_files = candidate_files


    
    status.info("Evaluating candidate profile (combined documents)")
    
    cv_results = []
    
    for job_idx, job in enumerate(jobs, start=1):
        status.info(f"Evaluating Job {job_idx}/{len(jobs)}")
    
        result = ai_match_job(candidate_files, job, SELECTED_MODEL)

    
        cv_results.append({
            "job": job,
            "score": result["data"]["score"],
            "criteria": result["data"]["criteria"]
        })
    
    cv_results.sort(key=lambda x: x["score"], reverse=True)
    
    st.session_state.results = [{
        "cv_name": "Combined Candidate Profile",
        "cv_type": "MULTI-DOC",
        "cv_files": cv_files,
        "results": cv_results
    }]
    
    status.success("Evaluation completed")
    progress.progress(1.0)

    
    

    
if st.session_state.results:
    st.subheader("CV Evaluation Results")


    for cv_idx, cv_block in enumerate(st.session_state.results):
        if not cv_block["results"]:
            continue

        best_job = cv_block["results"][0]
        st.success(
            f"Best match for {cv_block['cv_name']}: "
            f"**{best_job['job']['title']}** ({best_job['score']}%)"
        )

        st.markdown(f"## ğŸ“„ {cv_block['cv_name']} ({cv_block['cv_type']})")
        st.divider()
               
        candidate_container = st.container()
        
        with candidate_container:
            st.markdown("**Uploaded Documents:**")
            for name in cv_block.get("cv_files", []):
                st.markdown(f"- {name}")
        
            st.caption(
                "â€» ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå±¥æ­´æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€AIãŒç›´æ¥èª­ã¿å–ã‚Šãƒ»è©•ä¾¡ã—ã¦ã„ã¾ã™ã€‚"
                " äº‹å‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚„OCRå‡¦ç†ã¯è¡Œã£ã¦ã„ã¾ã›ã‚“ã€‚"
            )

        
            for job_idx, r in enumerate(cv_block["results"]):
                job = r["job"]
        
                st.markdown(f"### {job['title']}")
                st.write(f"**Estimated Offer Probability:** {r['score']}%")
        
                cols = st.columns(3)
                cols[0].markdown(f"**ä¼šç¤¾å**<br>{job['company_name']}", unsafe_allow_html=True)
                cols[1].markdown(
                    f"**æ›¸é¡é€šéç‡**<br>{job['passrate_for_doc_screening']}%",
                    unsafe_allow_html=True
                )
                cols[2].markdown(
                    f"**å†…å®šç‡**<br>{job['documents_to_job_offer_ratio']}",
                    unsafe_allow_html=True
                )

        
                explain_key = f"{cv_idx}_{job_idx}"
        
                if explain_key not in st.session_state.explain_open:
                    st.session_state.explain_open[explain_key] = False
        
                if st.button(
                    f"åˆ†æè©³ç´°ï¼ˆã“ã®è©•ä¾¡ã®ç†ç”±ï¼‰ â€“ {job['title']}",
                    key=f"explain_btn_{explain_key}"
                ):
                    st.session_state.explain_open[explain_key] = True

                    if explain_key not in st.session_state.explanations:
                        st.session_state.explanations[explain_key] = generate_explanation(job, r)

        
                if st.session_state.explain_open.get(explain_key, False):
                    sections = st.session_state.explanations.get(explain_key)
                    if sections:
                        st.markdown("### ğŸ“ è©•ä¾¡ã‚µãƒãƒªãƒ¼")
                        st.write(sections.get("SUMMARY", ""))
        
                        with st.expander("ğŸ“Š è©•ä¾¡è©³ç´°", expanded=True):

                            st.markdown("**å¿…é ˆè¦ä»¶ï¼ˆMust-haveï¼‰**")
                            st.write(sections.get("MUST_HAVE", ""))
                            st.markdown("**æ­“è¿è¦ä»¶ï¼ˆPreferredï¼‰**")
                            st.write(sections.get("PREFERRED", ""))
                            st.markdown("**æ¥­å‹™è¦ªå’Œæ€§ï¼ˆAlignmentï¼‰**")
                            st.write(sections.get("ALIGNMENT", ""))
        
        
        
                    
